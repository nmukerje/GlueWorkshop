{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buuilding a Data Lake using AWS Glue\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Activity 1 : CSV to Parquet conversion](#Activity-1-:-CSV-to-Parquet-conversion)\n",
    "3. [Activity 2 : Building a Star Schema in your Datalake](#Activity-2-:-Building-a-Star-Schema-in-your-Datalake)\n",
    "3. [Activity 3 : Building an AWS Glue Workflow](#Activity-3-:-Building-an-AWS-Glue-Workflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we will use AWS Glue to perform 2 activities:\n",
    "    \n",
    "- Convert a CSV Dataset to Parquet partitioned out by key fields.\n",
    "- Build a Star (Denormalized) Schema from an OLTP 3NF Schema.\n",
    "\n",
    "Let's start by connecting to our our Glue DevEndpoint - a persistent Glue Development environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 1 : CSV to Parquet conversion\n",
    "\n",
    "\n",
    "The 1st dataset we will be using is the NYC Taxi Trips CSV dataset with 1.2B records. We will partition the data in the analytics tier by vendor name, year and month, catalog this data in the AWS Glue Data Catalog. This dataset has 5 vendors and 8 years of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawl the Source Data\n",
    "\n",
    "The 1st step is to run the AWS Crawler on the raw dataset to create the table in the AWS Glue Catalog.\n",
    "\n",
    "Create and Execute a Glue Crawler on the source data in S3\n",
    "\n",
    "- Navigate to the Glue console at Services -> Glue\n",
    "- From the left-hand panel menu, navigate to Data Catalog -> Crawlers.\n",
    "- Click on the button ‘Add Crawler’ to create a new Glue Crawler.\n",
    "- Fields to fill in:\n",
    "    - Page: Add information about your crawler\n",
    "        - Crawler name: **nyc_trips_csv_crawler**\n",
    "    - Page: Add a data store\n",
    "        - Choose a data store: S3\n",
    "        - Include path: **s3://###s3_bucket###/data/nyc_trips_csv/**\n",
    "    - Page: Choose an IAM role\n",
    "       - IAM Role: Choose the **GlueServiceRole**\n",
    "    - Page: Configure the crawler's output\n",
    "        - Database: Click on ‘Add database’ and enter database name as **nyc_trips**.\n",
    "- Click on the button ‘Finish’ to create the crawler.\n",
    "- Select the new Crawler and click on 'Run crawler' to run the Crawler.\n",
    "\n",
    "Once the data is crawled, we can view the database and tables in the Glue Catalog and query the table as well:\n",
    "\n",
    "### Transform the data to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"use nyc_trips\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"select * from nyc_trips.nyc_trips_csv\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We will simulate the Glue job arguments \n",
    "import sys\n",
    "sys.argv = [\"CSV2Parquet\",\"--JOB_NAME\", \"CSV2Parquet\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start the code for the Glue Job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Glue boilerplate code\n",
    "\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "import boto3, json\n",
    "\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "print (args['JOB_NAME']+\" START...\")\n",
    "if 'sc' not in vars(): sc = SparkContext()\n",
    "glueContext = GlueContext(SparkContext.getOrCreate())\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "## Glue boilerplate code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name='nyc_trips'\n",
    "tbl_name='nyc_trips_csv'\n",
    "output_dir='s3://###s3_bucket###/data/nyc_trips_parquet/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyf = glueContext.create_dynamic_frame.from_catalog(database = db_name, table_name = tbl_name)\n",
    "dyf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the data out in Parquet\n",
    "glueContext.write_dynamic_frame.from_options(frame = dyf, connection_type = \"s3\", connection_options = {\"path\": output_dir, \"partitionKeys\": ['vendor_name', 'year', 'month']}, format = \"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawl the Transformed Data\n",
    "\n",
    "Now that the output data is in Amazon S3, let's crawl this dataset in AWS Glue and query this data using Amazon Athena.\n",
    "\n",
    "- Navigate to the Glue console at Services -> Glue\n",
    "- From the left-hand panel menu, navigate to Data Catalog -> Crawlers.\n",
    "- Click on the button ‘Add Crawler’ to create a new Glue Crawler.\n",
    "- Fields to fill in:\n",
    "    - Page: Add information about your crawler\n",
    "        - Crawler name: **nyc_trips_parquet_crawler**\n",
    "    - Page: Add a data store\n",
    "        - Choose a data store: S3\n",
    "        - Include path: **s3://###s3_bucket###/data/nyc_trips_parquet/**\n",
    "    - Page: Choose an IAM role\n",
    "       - IAM Role: Choose the **GlueServiceRole**\n",
    "    - Page: Configure the crawler's output\n",
    "        - Database: Select database as **nyc_trips**\n",
    "- Click on the button ‘Finish’ to create the crawler.\n",
    "- Select the new Crawler and click on Run crawler to run the Crawler.\n",
    "\n",
    "Navigate to the Athena console, Services -> Athena to run Athena queries on this dataset.\n",
    "\n",
    "Note: You may set output location for Athena by clicking on Settings -> Query result location\n",
    "and setting the value to : \n",
    "\n",
    "**s3://###s3_bucket###/athena-query-results/**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 2 : Building a Star Schema in your Datalake\n",
    "\n",
    "In this activity, we will denormalize an OLTP 3NF schema to Parquet. This activity demonstrates the using Glue operations to perform powerful data transformations on input data:\n",
    "\n",
    "![alt text](resources/denormalize.png \"Building a Star Schema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawl the Source Data\n",
    "\n",
    "The 1st step is to run the AWS Crawler on the raw dataset to create the tables in the AWS Glue Catalog.\n",
    "\n",
    "- Navigate to the Glue console at Services -> Glue\n",
    "- From the left-hand panel menu, navigate to Data Catalog -> Crawlers.\n",
    "- Click on the button ‘Add Crawler’ to create a new Glue Crawler.\n",
    "- Fields to fill in:\n",
    "    - Page: Add information about your crawler\n",
    "        - Crawler name: **salesdb_crawler**\n",
    "    - Page: Add a data store\n",
    "        - Choose a data store: S3\n",
    "        - Include path: **s3://###s3_bucket###/data/salesdb/**\n",
    "    - Page: Choose an IAM role\n",
    "       - IAM Role: Choose the **GlueServiceRole**\n",
    "    - Page: Configure the crawler's output\n",
    "        - Database:  Click on ‘Add database’ and enter database name as **salesdb**.\n",
    "- Click on the button ‘Finish’ to create the crawler.\n",
    "- Select the new Crawler and click on Run crawler to run the Crawler.\n",
    "\n",
    "### Transform the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name='salesdb'\n",
    "table1='customer'\n",
    "table2='customer_site'\n",
    "output_dir='s3://###s3_bucket###/data/sales_analytics/customer_dim'\n",
    "print (output_dir)\n",
    "\n",
    "# Read the Source Tables\n",
    "cust_dyf = glueContext.create_dynamic_frame.from_catalog(database = db_name, table_name = table1)\n",
    "cust_site_dyf = glueContext.create_dynamic_frame.from_catalog(database = db_name, table_name = table2)\n",
    "\n",
    "#Join the Source Tables\n",
    "customer_dim_dyf = Join.apply(cust_dyf,cust_site_dyf,\n",
    "                       'cust_id', 'cust_id').drop_fields(['cust_id'])\n",
    "\n",
    "# Write the denormalized CUSTOMER_DIM table in Parquet\n",
    "glueContext.write_dynamic_frame.from_options(frame = customer_dim_dyf, connection_type = \"s3\", connection_options = {\"path\": output_dir}, format = \"parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1='product_category'\n",
    "table2='product'\n",
    "output_dir='s3://###s3_bucket###/data/sales_analytics/product_dim/'\n",
    "print (output_dir)\n",
    "\n",
    "# Read the Source Tables\n",
    "table1_dyf = glueContext.create_dynamic_frame.from_catalog(database = db_name, table_name = table1)\n",
    "table2_dyf = glueContext.create_dynamic_frame.from_catalog(database = db_name, table_name = table2)\n",
    "\n",
    "#Join the Source Tables\n",
    "product_dim_dyf = Join.apply(table1_dyf,table2_dyf,\n",
    "                       'category_id', 'category_id').drop_fields(['category_id'])\n",
    "\n",
    "# Write the denormalized CUSTOMER_DIM table in Parquet\n",
    "glueContext.write_dynamic_frame.from_options(frame = product_dim_dyf, connection_type = \"s3\", connection_options = {\"path\": output_dir}, format = \"parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1='supplier'\n",
    "output_dir='s3://###s3_bucket###/data/sales_analytics/supplier_dim/'\n",
    "print (output_dir)\n",
    "\n",
    "# Read the Source Tables\n",
    "table1_dyf = glueContext.create_dynamic_frame.from_catalog(database = db_name, table_name = table1)\n",
    "\n",
    "\n",
    "# Write the denormalized CUSTOMER_DIM table in Parquet\n",
    "glueContext.write_dynamic_frame.from_options(frame = table1_dyf, connection_type = \"s3\", connection_options = {\"path\": output_dir}, format = \"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1='sales_order_detail'\n",
    "table2='sales_order'\n",
    "output_dir='s3://###s3_bucket###/data/sales_analytics/sales_order_fact/'\n",
    "print (output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the 'sales_order_fact' table, we will try a different approach - \n",
    "\n",
    "- We will convert the Glue DynamicFrame to a Spark DataFrame\n",
    "- Register the Spark Dataframe to a Spark Temporary View\n",
    "- Use Spark SQL to build the write out the target dataset.\n",
    "\n",
    "This shows that Glue DynamicFrames and Spark Dataframes are interchangeable and you can get the best of both worlds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Source Tables\n",
    "table1_dyf = glueContext.create_dynamic_frame.from_catalog(database = db_name, table_name = table1)\n",
    "table2_dyf = glueContext.create_dynamic_frame.from_catalog(database = db_name, table_name = table2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1_dyf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2_dyf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1_dyf.toDF().createOrReplaceTempView(\"sales_order_v\")\n",
    "table2_dyf.toDF().createOrReplaceTempView(\"sales_order_detail_v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the denormalized SALES_ORDER_FACT table\n",
    "df=spark.sql(\"SELECT a.*, b.site_id, b.order_date,b.ship_mode \\\n",
    "FROM sales_order_detail_v b, sales_order_v a \\\n",
    "WHERE a.order_id=b.order_id\")\n",
    "df.printSchema()\n",
    "print(df.count())\n",
    "df.coalesce(1).write.mode(\"OVERWRITE\").parquet(\"s3://###s3_bucket###/data/sales_analytics/sales_order_fact/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the outut data is in Amazon S3, let's crawl this dataset in AWS Glue and query this data using Amazon Athena."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawl the Transformed Data\n",
    "\n",
    "- Navigate to the Glue console at Services -> Glue\n",
    "- From the left-hand panel menu, navigate to Data Catalog -> Crawlers.\n",
    "- Click on the button ‘Add Crawler’ to create a new Glue Crawler.\n",
    "- Fields to fill in:\n",
    "    - Page: Add information about your crawler\n",
    "        - Crawler name: **sales_analytics_crawler**\n",
    "    - Page: Add a data store\n",
    "        - Choose a data store: S3\n",
    "        - Include path: **s3://###s3_bucket###/data/sales_analytics/**\n",
    "    - Page: Choose an IAM role\n",
    "       - IAM Role: Choose the **GlueServiceRole**\n",
    "    - Page: Configure the crawler's output\n",
    "        - Database:  Click on ‘Add database’ and enter database name as **sales_analytics**.\n",
    "- Click on the button ‘Finish’ to create the crawler.\n",
    "- Select the new Crawler and click on Run crawler to run the Crawler.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 3 : Building an AWS Glue Workflow\n",
    "\n",
    "Let's now build an AWS Glue Workflow for the same. The 1st step is to create the Glue Jobs. As the code is already available, we will simply call the AWS Glue APIs to create the Glue Jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%local\n",
    "\n",
    "import boto3\n",
    "\n",
    "acct_number=boto3.client('sts').get_caller_identity().get('Account')\n",
    "bucket='###s3_bucket###'\n",
    "\n",
    "# Create the Glue Spark Jobs\n",
    "glue = boto3.client(\"glue\")\n",
    "\n",
    "for job_name in ['Load_SALES_ORDER_FACT', 'Load_PRODUCT_DIM', 'Load_CUSTOMER_DIM','Load_SUPPLIER_DIM']:\n",
    "    response=glue.create_job(Name=job_name,\n",
    "                         Role=\"arn:aws:iam::%s:role/GlueServiceRole\"%acct_number,\n",
    "                         ExecutionProperty={'MaxConcurrentRuns': 1},\n",
    "                         Command={'Name': 'glueetl',\n",
    "                                  'ScriptLocation': 's3://%s/scripts/glue/%s.py'%(bucket,job_name),\n",
    "                                  'PythonVersion': '3'},\n",
    "                         DefaultArguments={'--TempDir': 's3://%s/temp'%bucket,\n",
    "                                           '--enable-continuous-cloudwatch-log': 'true',\n",
    "                                           '--enable-glue-datacatalog': '',\n",
    "                                           '--enable-metrics': '',\n",
    "                                           '--enable-spark-ui': 'true',\n",
    "                                           '--spark-event-logs-path': 's3://%s/spark_glue_etl_logs/%s'%(bucket,job_name),\n",
    "                                           '--job-bookmark-option': 'job-bookmark-disable',\n",
    "                                           '--job-language': 'python',\n",
    "                                           '--S3_BUCKET': bucket },\n",
    "                         MaxRetries=0,\n",
    "                         Timeout=2880,\n",
    "                         MaxCapacity=3.0,\n",
    "                         GlueVersion='1.0',\n",
    "                         Tags={'Owner': 'TKO_Labs'}\n",
    "                        )\n",
    "    print (response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the job is created, lets us build an AWS Glue Workflow for the same:\n",
    "    \n",
    "- Navigate to the Glue Console at Service -> Glue\n",
    "- From the left-hand panel menu, choose Workflows and click on 'Add Workflow'\n",
    "- Enter WorkFlow Name as 'Data_Transformation_Workflow' and click on 'Add Workflow'.\n",
    "\n",
    "To create the Workflow:\n",
    "\n",
    "- Trigger 1_Load_Dimensions:\n",
    "  - Click on 'Add Trigger'\n",
    "  - Select Tab 'Add New'\n",
    "  - Enter Trigger Name as '1_Load_Dimensions'\n",
    "  - Choose Trigger type as 'On demand'\n",
    "  - Click on 'Add' to create the Trigger\n",
    "  - Select the trigger and click on 'Add Action' -> 'Add jobs/crawlers to trigger'\n",
    "  - Select the jobs - Load_CUSTOMER_DIM, Load_PRODUCT_DIM and Load_SUPPLIER_DIM\n",
    "  - Click on 'Add' to save changes\n",
    "- Trigger 2_Load_Facts:\n",
    "  - Click on any of the jobs e.g. 'Load_CUSTOMER_DIM'\n",
    "  - Click on 'Add Trigger'\n",
    "  - Select Tab 'Add New'\n",
    "  - Enter Trigger Name as '2_Load_Facts'\n",
    "  - Choose Trigger logic as 'Start after ALL watched event'\n",
    "  - Click on 'Add' to create the Trigger\n",
    "  - Select the trigger and click on 'Add Action' -> 'Add jobs/crawlers to watch'\n",
    "  - Select the other jobs that are part of the load dimensions step -  Load_PRODUCT_DIM and Load_SUPPLIER_DIM\n",
    "  - Click on 'Add' to save changes\n",
    "  - Select the trigger and click on 'Add Action' -> 'Add jobs/crawlers to trigger'\n",
    "  - Select the job - Load_SALES_ORDER_FACT\n",
    "  - Click on 'Add' to save changes\n",
    "- Trigger 3_Update_Metadata:\n",
    "  - Click on the job e.g. 'Load_SALES_ORDER_FACT'\n",
    "  - Click on 'Add Trigger'\n",
    "  - Select Tab 'Add New'\n",
    "  - Enter Trigger Name as '3_Update_Metadata'\n",
    "  - Click on 'Add' to create the Trigger\n",
    "  - Select the trigger and click on 'Add Action' -> 'Add jobs/crawlers to trigger'\n",
    "  - From the 'Crawlers' tab, select the crawler 'sales_analytics_crawler'.\n",
    "  - Click on 'Add' to save changes\n",
    "  \n",
    "Your workflow should look like this:\n",
    "\n",
    "![title](resources/Glue_Workflow.png)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Glue Workflow is complete. Let us now run this workflow: \n",
    "\n",
    "- Select the workflow and click on Action - > Run\n",
    "- You can view the run details from the 'History' tab by selecting the workflow run and clicking on 'View Run Details'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-up\n",
    "\n",
    "In this notebook, we ran exercises to perform: \n",
    "\n",
    "1. A CSV to Parquet conversion and observed how easy it is to transform and write data to S3 using AWS Glue, partitioned by key fields.\n",
    "2. A more complex transformation - denormalizing of a 3NF OLTP schema, and we observed how easy it is to perform complex data transformations using the power of both Glue DynamicFrames and Spark SQL.\n",
    "3. We built and executed an AWS Glue Workflow to orchestrate the Glue Jobs.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
