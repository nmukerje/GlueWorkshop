{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using AWS Glue Python Shell Jobs\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Activity 1 : Executing Amazon Athena Queries](#Activity_1_:_Executing-Amazon-Athena-Queries)\n",
    "3. [Activity 2 : Deploying the AWS Glue Python Shell Job](#Activity-2-:-Deploying-the-AWS-Glue-Python-Shell-Job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we are going to explore using AWS Glue Python Shell Jobs. Not every use case needs the power of Apache Spark, and Python is a vert versatile framework for data processing. Use cases where AWS Glue Python Shell jobs can be used are:\n",
    "    \n",
    "- Orchestrating SQL in databases like Redshift, Aurora etc.\n",
    "- Light-weight ETL using Amazon Athena.\n",
    "- Data processing using Python Pandas or Numpy libraries.\n",
    "- Building Python ML models using Python Scikit-Learn.\n",
    "- And anything else that Python can accomplish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 1 : Executing Amazon Athena Queries\n",
    "\n",
    "We will deploy a Light-weight ETL pipeline by simply executing a SQL Script in Amazon Athena using AWS Glue Python Shell Jobs in this lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3,time\n",
    "import pandas as pd\n",
    "\n",
    "region='eu-west-1'\n",
    "defaultdb=\"default\"\n",
    "\n",
    "default_output='s3://glue-labs-959874710265/athena-sql/data/output/'\n",
    "default_write_location='s3://glue-labs-959874710265/athena-sql/data/'\n",
    "default_script_location= '3://glue-labs-959874710265/athena-sql/scripts/'\n",
    "default_script_logs_location = 's3://glue-labs-959874710265/athena-sql/logs/'\n",
    "sql_script_file='athena-sql-script.sql'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will write a simple helper function that allows us to send SQL statement to Amazon Athena:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def executeQuery(query, database=defaultdb, s3_output=default_output, poll=10):\n",
    "    log_output (\"Executing Query : \\n\") \n",
    "    start = time.time()\n",
    "    log_output (query+\"\\n\")\n",
    "    athena = boto3.client('athena',region_name=region)\n",
    "    response = athena.start_query_execution(\n",
    "        QueryString=query,\n",
    "        QueryExecutionContext={\n",
    "            'Database': database\n",
    "            },\n",
    "        ResultConfiguration={\n",
    "            'OutputLocation': s3_output,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    log_output('Execution ID: ' + response['QueryExecutionId'])\n",
    "    queryExecutionId=response['QueryExecutionId']\n",
    "    state='QUEUED'\n",
    "    while( state=='RUNNING' or state=='QUEUED'):\n",
    "        response = athena.get_query_execution(QueryExecutionId=queryExecutionId)\n",
    "        state=response['QueryExecution']['Status']['State']\n",
    "        log_output (state)\n",
    "        if  state=='RUNNING' or state=='QUEUED':\n",
    "            time.sleep(poll)\n",
    "        elif (state=='FAILED'):\n",
    "             log_output (response['QueryExecution']['Status']['StateChangeReason'])\n",
    "              \n",
    "    done = time.time()\n",
    "    log_output (\"Elapsed Time (in seconds) : %f \\n\"%(done - start))\n",
    "    return response\n",
    "\n",
    "def log_output(s):\n",
    "    log_output_string.append(s)\n",
    "    \n",
    "def read_from_athena(sql):\n",
    "    response=executeQuery(sql)\n",
    "    return pd.read_csv(response['QueryExecution']['ResultConfiguration']['OutputLocation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script we are going to use is here : [athena-sql-script.sql](athena-sql-script.sql) \n",
    "\n",
    "We will read the SQL file from our S3 bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_location= default_script_location+sql_script_file\n",
    "bucket_name,script_location=s3_location.split('/',2)[2].split('/',1)\n",
    "print (bucket_name)\n",
    "print (script_location)\n",
    "\n",
    "s3 = boto3.client('s3',region_name='eu-west-1')\n",
    "\n",
    "fileobj = s3.get_object(Bucket=bucket_name,Key=script_location)\n",
    "contents = fileobj['Body'].read().decode('utf-8')\n",
    "contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's execute the script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_output_string=[]\n",
    "for sql in str(contents).split(\";\")[:-1]:\n",
    "    response=executeQuery(sql)\n",
    "print (\"\\n\".join(log_output_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our pipeline is complete and we can see the results of our SQL Script run. Let us read the final report data as a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_from_athena(\"Select * from default.nyc_top_trips_report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 2 : Deploying the AWS Glue Python Shell Job\n",
    "\n",
    "As a final step, we will deploy this pipeline as an AWS Glue Python Shell job and execute it.\n",
    "\n",
    "Note that an AWS Glue Python Shell job can use 1 DPU (Data Processing Unit) or 0.0625 DPU (which is 1/16 DPU). A single DPU provides processing capacity that consists of 4 vCPUs of compute and 16 GB of memory. For our use case, 0.0625 DPU is sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "acct_number=boto3.client('sts').get_caller_identity().get('Account')\n",
    "bucket='glue-labs-959874710265'\n",
    "\n",
    "# Create the Glue Spark Jobs\n",
    "glue = boto3.client(\"glue\")\n",
    "\n",
    "for job_name in ['Build_NYC_Top_Trips_Report']:\n",
    "    response=glue.create_job(Name=job_name,\n",
    "                         Role=\"arn:aws:iam::%s:role/GlueServiceRole\"%acct_number,\n",
    "                         ExecutionProperty={'MaxConcurrentRuns': 1},\n",
    "                         Command={'Name': 'pythonshell',\n",
    "                                  'ScriptLocation': 's3://%s/athena-sql/scripts/%s.py'%(bucket,job_name),\n",
    "                                  'PythonVersion': '3'},\n",
    "                         DefaultArguments={'--TempDir': 's3://%s/temp'%bucket,\n",
    "                                           '--enable-metrics': '',\n",
    "                                           '--job-language': 'python',\n",
    "                                           '--S3_BUCKET': bucket },\n",
    "                         MaxRetries=0,\n",
    "                         Timeout=2880,\n",
    "                         MaxCapacity=0.0625,\n",
    "                         GlueVersion='1.0',\n",
    "                         Tags={'Owner': 'AWS_Glue_Labs'}\n",
    "                        )\n",
    "    print (response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the AWS Ge itlue job is deployed, let's execute it:\n",
    "\n",
    "- Navigate to the AWS Glue Console -> Jobs. \n",
    "- Select the 'Build_NYC_Top_Trips_Report' Glue Jobs and \n",
    "- Click on the Action -> Run Job option to execute the job.  \n",
    "\n",
    "We can monitor the Execution Details from the AWS Glue console and once the job is over view the logs by clicking on the 'Logs' link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-up\n",
    "\n",
    "In this notebook, we ran exercises to : \n",
    "\n",
    "1. Execute a light-weight SQL driven ETL pipeline using Amazon Athena and\n",
    "2. Deployed the pipeline as a AWS Glue Python Shell Job.\n",
    "\n",
    "We hope this lab helped you to understand how to leverage the simplicity and power of Python in your Data Pipelines using AWS Glue Python Shell Jobs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
