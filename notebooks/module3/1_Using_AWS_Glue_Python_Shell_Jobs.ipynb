{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using AWS Glue Python Shell Jobs\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Activity 1 : Executing Amazon Athena Queries](#Activity_1_:_Executing-Amazon-Athena-Queries)\n",
    "3. [Activity 2 : Deploying the AWS Glue Python Shell Job](#Activity-2-:-Deploying-the-AWS-Glue-Python-Shell-Job)\n",
    "4. [Wrap-up](#Wrap-up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we are going to explore using AWS Glue Python Shell Jobs. Not every use case needs the power of Apache Spark, and Python is a vert versatile framework for data processing. Use cases where AWS Glue Python Shell jobs can be used are:\n",
    "    \n",
    "- Orchestrating SQL in databases like Redshift, Aurora etc.\n",
    "- Light-weight ETL using Amazon Athena.\n",
    "- Data processing using Python Pandas or Numpy libraries.\n",
    "- Building Python ML models using Python Scikit-Learn.\n",
    "- And anything else that Python can accomplish.\n",
    "\n",
    "Let's build a SQL driven ETL pipeline that uses the power of Amazon Athena to execute SQL scripts over an Amazon S3 data lake.\n",
    "\n",
    "The architecture diagram for this module looks like below:\n",
    "\n",
    "\n",
    "<img src=\"../resources/module3_architecture_diagram.png\" alt=\"Module3 Architecture Diagram]\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 1 : Executing Amazon Athena Queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T16:25:24.666274Z",
     "start_time": "2020-05-07T16:25:19.877248Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3,time\n",
    "import pandas as pd\n",
    "\n",
    "defaultdb=\"default\"\n",
    "\n",
    "default_output='s3://###s3_bucket###/athena-sql/data/output/'\n",
    "default_write_location='s3://###s3_bucket###/athena-sql/data/'\n",
    "default_script_location= '3://###s3_bucket###/scripts/'\n",
    "default_script_logs_location = 's3://###s3_bucket###/athena-sql/logs/'\n",
    "sql_script_file='athena-sql-script.sql'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will write a simple helper function that allows us to send SQL statement to Amazon Athena:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T16:25:28.043780Z",
     "start_time": "2020-05-07T16:25:28.035937Z"
    }
   },
   "outputs": [],
   "source": [
    "def executeQuery(query, database=defaultdb, s3_output=default_output, poll=10):\n",
    "    log_output (\"Executing Query : \\n\") \n",
    "    start = time.time()\n",
    "    log_output (query+\"\\n\")\n",
    "    athena = boto3.client('athena')\n",
    "    response = athena.start_query_execution(\n",
    "        QueryString=query,\n",
    "        QueryExecutionContext={\n",
    "            'Database': database\n",
    "            },\n",
    "        ResultConfiguration={\n",
    "            'OutputLocation': s3_output,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    log_output('Execution ID: ' + response['QueryExecutionId'])\n",
    "    queryExecutionId=response['QueryExecutionId']\n",
    "    state='QUEUED'\n",
    "    while( state=='RUNNING' or state=='QUEUED'):\n",
    "        response = athena.get_query_execution(QueryExecutionId=queryExecutionId)\n",
    "        state=response['QueryExecution']['Status']['State']\n",
    "        log_output (state)\n",
    "        if  state=='RUNNING' or state=='QUEUED':\n",
    "            time.sleep(poll)\n",
    "        elif (state=='FAILED'):\n",
    "             log_output (response['QueryExecution']['Status']['StateChangeReason'])\n",
    "              \n",
    "    done = time.time()\n",
    "    log_output (\"Elapsed Time (in seconds) : %f \\n\"%(done - start))\n",
    "    return response\n",
    "\n",
    "def log_output(s):\n",
    "    log_output_string.append(s)\n",
    "    \n",
    "def read_from_athena(sql):\n",
    "    response=executeQuery(sql)\n",
    "    return pd.read_csv(response['QueryExecution']['ResultConfiguration']['OutputLocation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script we are going to use is here : [athena-sql-script.sql](athena-sql-script.sql) \n",
    "\n",
    "We will read the SQL file from our S3 bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T16:25:31.971898Z",
     "start_time": "2020-05-07T16:25:31.495175Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###s3_bucket###\n",
      "scripts/athena-sql-script.sql\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"--\\n-- Athena SQL script\\n--\\n\\n-- Drop table\\nDROP TABLE default.nyc_trips_pq_1;\\n\\n-- Create table\\nCREATE EXTERNAL TABLE `nyc_trips_pq_1`(\\n  `vendor_name` string, \\n  `trip_pickup_datetime` string, \\n  `trip_dropoff_datetime` string, \\n  `passenger_count` int, \\n  `trip_distance` float, \\n  `payment_type` string, \\n  `are_amt` float, \\n  `surcharge` float, \\n  `mta_tax` float, \\n  `tip_amt` float, \\n  `tolls_amt` float, \\n  `total_amt` float)\\nPARTITIONED BY ( \\n  `year` string, \\n  `month` string)\\nROW FORMAT SERDE \\n  'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' \\nSTORED AS INPUTFORMAT \\n  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat' \\nOUTPUTFORMAT \\n  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\\nLOCATION\\n  's3://neilawspublic/dataset2'\\nTBLPROPERTIES (\\n  'parquet.compress'='SNAPPY');\\n\\n-- Load the partitions\\nMSCK REPAIR TABLE nyc_trips_pq_1;\\n\\n-- Drop the Report table.\\nDROP TABLE default.nyc_top_trips_report;\\n\\n-- Create the Report table.\\nCREATE TABLE default.nyc_top_trips_report\\nWITH (\\n  format='PARQUET'\\n) AS\\nSelect year, month,\\nsum(Passenger_Count) as total_passengers, count(1) as total_trips\\nfrom nyc_trips_pq_1\\ngroup by year, month\\norder by 4 DESC\\nLIMIT 10;\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_location= default_script_location+sql_script_file\n",
    "bucket_name,script_location=s3_location.split('/',2)[2].split('/',1)\n",
    "print (bucket_name)\n",
    "print (script_location)\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "fileobj = s3.get_object(Bucket=bucket_name,Key=script_location)\n",
    "contents = fileobj['Body'].read().decode('utf-8')\n",
    "contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's execute the script. This step issues the SQL commands to Amazon Athena and should take around 2 mins. You can navigate to the Amazon Athena console and view the queries being submitted:\n",
    "\n",
    "- Navigate to the AWS Athena Console\n",
    "- Click on the History tabe to view the queries submitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T16:27:46.338827Z",
     "start_time": "2020-05-07T16:25:35.278713Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing Query : \n",
      "\n",
      "--\n",
      "-- Athena SQL script\n",
      "--\n",
      "\n",
      "-- Drop table\n",
      "DROP TABLE default.nyc_trips_pq_1\n",
      "\n",
      "Execution ID: 684cdef4-6646-4495-ae9b-1cea3f158900\n",
      "QUEUED\n",
      "SUCCEEDED\n",
      "Elapsed Time (in seconds) : 10.257477 \n",
      "\n",
      "Executing Query : \n",
      "\n",
      "\n",
      "\n",
      "-- Create table\n",
      "CREATE EXTERNAL TABLE `nyc_trips_pq_1`(\n",
      "  `vendor_name` string, \n",
      "  `trip_pickup_datetime` string, \n",
      "  `trip_dropoff_datetime` string, \n",
      "  `passenger_count` int, \n",
      "  `trip_distance` float, \n",
      "  `payment_type` string, \n",
      "  `are_amt` float, \n",
      "  `surcharge` float, \n",
      "  `mta_tax` float, \n",
      "  `tip_amt` float, \n",
      "  `tolls_amt` float, \n",
      "  `total_amt` float)\n",
      "PARTITIONED BY ( \n",
      "  `year` string, \n",
      "  `month` string)\n",
      "ROW FORMAT SERDE \n",
      "  'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' \n",
      "STORED AS INPUTFORMAT \n",
      "  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat' \n",
      "OUTPUTFORMAT \n",
      "  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\n",
      "LOCATION\n",
      "  's3://neilawspublic/dataset2'\n",
      "TBLPROPERTIES (\n",
      "  'parquet.compress'='SNAPPY')\n",
      "\n",
      "Execution ID: c278c1c9-b059-4983-9216-d6b333ea0158\n",
      "QUEUED\n",
      "SUCCEEDED\n",
      "Elapsed Time (in seconds) : 10.138438 \n",
      "\n",
      "Executing Query : \n",
      "\n",
      "\n",
      "\n",
      "-- Load the partitions\n",
      "MSCK REPAIR TABLE nyc_trips_pq_1\n",
      "\n",
      "Execution ID: 7d379776-d39e-46b3-a2f7-56227443dd16\n",
      "QUEUED\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "SUCCEEDED\n",
      "Elapsed Time (in seconds) : 80.366224 \n",
      "\n",
      "Executing Query : \n",
      "\n",
      "\n",
      "\n",
      "-- Drop the Report table.\n",
      "DROP TABLE default.nyc_top_trips_report\n",
      "\n",
      "Execution ID: fe4ad2bc-0561-46f4-b54c-96b1f8234835\n",
      "QUEUED\n",
      "SUCCEEDED\n",
      "Elapsed Time (in seconds) : 10.117631 \n",
      "\n",
      "Executing Query : \n",
      "\n",
      "\n",
      "\n",
      "-- Create the Report table.\n",
      "CREATE TABLE default.nyc_top_trips_report\n",
      "WITH (\n",
      "  format='PARQUET'\n",
      ") AS\n",
      "Select year, month,\n",
      "sum(Passenger_Count) as total_passengers, count(1) as total_trips\n",
      "from nyc_trips_pq_1\n",
      "group by year, month\n",
      "order by 4 DESC\n",
      "LIMIT 10\n",
      "\n",
      "Execution ID: 2bab0cd8-6375-4725-b0bc-79669d63b167\n",
      "QUEUED\n",
      "RUNNING\n",
      "SUCCEEDED\n",
      "Elapsed Time (in seconds) : 20.170374 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_output_string=[]\n",
    "for sql in str(contents).split(\";\")[:-1]:\n",
    "    response=executeQuery(sql)\n",
    "print (\"\\n\".join(log_output_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our pipeline is complete and we can see the results of our SQL Script run above - the Amazon Athena Execution Ids of each query as well the execution time for each query.\n",
    "\n",
    "Python shell jobs in AWS Glue come pre-loaded with libraries such as the Boto3, NumPy, SciPy, Pandas and others. You can load any custom Python libraries into the AWS Glue Python Shell environment packaged as an .egg or a .whl file.\n",
    "\n",
    "You can read more about AWS Glue Python Shell features here: https://docs.aws.amazon.com/glue/latest/dg/add-job-python.html\n",
    "\n",
    "Let us read the final report data as a Pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T16:30:18.745949Z",
     "start_time": "2020-05-07T16:30:08.373115Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>total_passengers</th>\n",
       "      <th>total_trips</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>26866837</td>\n",
       "      <td>16146923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011</td>\n",
       "      <td>3</td>\n",
       "      <td>26091246</td>\n",
       "      <td>16066350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>26965079</td>\n",
       "      <td>15749228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011</td>\n",
       "      <td>10</td>\n",
       "      <td>26287953</td>\n",
       "      <td>15707756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009</td>\n",
       "      <td>10</td>\n",
       "      <td>26202049</td>\n",
       "      <td>15604551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2012</td>\n",
       "      <td>5</td>\n",
       "      <td>26278817</td>\n",
       "      <td>15567525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2011</td>\n",
       "      <td>5</td>\n",
       "      <td>25508952</td>\n",
       "      <td>15554868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2010</td>\n",
       "      <td>9</td>\n",
       "      <td>25533166</td>\n",
       "      <td>15540209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2010</td>\n",
       "      <td>5</td>\n",
       "      <td>26002858</td>\n",
       "      <td>15481351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2012</td>\n",
       "      <td>4</td>\n",
       "      <td>25900645</td>\n",
       "      <td>15477914</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  total_passengers  total_trips\n",
       "0  2012      3          26866837     16146923\n",
       "1  2011      3          26091246     16066350\n",
       "2  2013      3          26965079     15749228\n",
       "3  2011     10          26287953     15707756\n",
       "4  2009     10          26202049     15604551\n",
       "5  2012      5          26278817     15567525\n",
       "6  2011      5          25508952     15554868\n",
       "7  2010      9          25533166     15540209\n",
       "8  2010      5          26002858     15481351\n",
       "9  2012      4          25900645     15477914"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_from_athena(\"Select * from default.nyc_top_trips_report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 2 : Deploying the AWS Glue Python Shell Job\n",
    "\n",
    "As a final step, we will deploy this pipeline as an AWS Glue Python Shell job and execute it.\n",
    "\n",
    "Note that an AWS Glue Python Shell job can use 1 DPU (Data Processing Unit) or 0.0625 DPU (which is 1/16 DPU). A single DPU provides processing capacity that consists of 4 vCPUs of compute and 16 GB of memory. For our use case, 0.0625 DPU is sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T16:46:06.017969Z",
     "start_time": "2020-05-07T16:46:05.515220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Name': 'Build_Top_Flight_Delays_Report', 'ResponseMetadata': {'RequestId': '6c8ff977-577a-42a1-bb58-b50544585d16', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 07 May 2020 16:46:06 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '41', 'connection': 'keep-alive', 'x-amzn-requestid': '6c8ff977-577a-42a1-bb58-b50544585d16'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "acct_number=boto3.client('sts').get_caller_identity().get('Account')\n",
    "bucket='###s3_bucket###'\n",
    "\n",
    "# Create the Glue Spark Jobs\n",
    "glue = boto3.client(\"glue\")\n",
    "\n",
    "for job_name in ['Build_Top_Flight_Delays_Report']:\n",
    "    response=glue.create_job(Name=job_name,\n",
    "                         Role=\"arn:aws:iam::%s:role/###iam_role###\"%acct_number,\n",
    "                         ExecutionProperty={'MaxConcurrentRuns': 1},\n",
    "                         Command={'Name': 'pythonshell',\n",
    "                                  'ScriptLocation': 's3://%s/scripts/%s.py'%(bucket,job_name),\n",
    "                                  'PythonVersion': '3'},\n",
    "                         DefaultArguments={'--TempDir': 's3://%s/temp'%bucket,\n",
    "                                           '--enable-metrics': '',\n",
    "                                           '--job-language': 'python',\n",
    "                                           '--S3_BUCKET': bucket },\n",
    "                         MaxRetries=0,\n",
    "                         Timeout=2880,\n",
    "                         MaxCapacity=0.0625,\n",
    "                         GlueVersion='1.0',\n",
    "                         Tags={'Owner': 'AWS_Glue_Labs'}\n",
    "                        )\n",
    "    print (response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the AWS Glue job is deployed, let's execute it:\n",
    "\n",
    "- Navigate to the AWS Glue Console -> Jobs. \n",
    "- Select the 'Build_Top_Flight_Delays_Report' Glue Jobs and \n",
    "- Click on the 'Action -> Run Job' option to execute the job.  \n",
    "\n",
    "We can monitor the Execution Details from the AWS Glue console and once the job is over view the logs by clicking on the 'Logs' link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-up\n",
    "\n",
    "In this notebook, we ran exercises to : \n",
    "\n",
    "1. Execute a light-weight SQL driven ETL pipeline using Amazon Athena and\n",
    "2. Deployed the pipeline as a AWS Glue Python Shell Job.\n",
    "\n",
    "We hope this lab helped you to understand how to leverage the simplicity and power of Python in your Data Pipelines using AWS Glue Python Shell Jobs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
